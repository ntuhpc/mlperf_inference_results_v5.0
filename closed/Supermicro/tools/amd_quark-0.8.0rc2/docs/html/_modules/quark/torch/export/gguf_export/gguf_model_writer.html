
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.export.gguf_export.gguf_model_writer &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/export/gguf_export/gguf_model_writer';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ‚ñº\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ‚ñº\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ‚ñ∂";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.export.gguf_export.gguf_model_writer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.export.gguf_export.gguf_model_writer</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#
# Copyright (c) 2023-2024 The ggml authors
# SPDX-License-Identifier: MIT
#

from __future__ import annotations

import json
import os
from abc import ABC, abstractmethod
from enum import IntEnum
from pathlib import Path
from hashlib import sha256
from typing import Any, Callable, ContextManager, Iterator, Sequence, TypeVar, cast, Dict

import torch
from .tensor_convert import convert_to_gguf
from quark.shares.utils.log import ScreenLogger, log_errors
from quark.shares.utils.import_utils import is_transformers_available

if is_transformers_available():
    from transformers import AutoTokenizer

logger = ScreenLogger(__name__)

try:
    import gguf  # type: ignore
except ImportError as e:
    logger.exception(str(e))
    raise ImportError(&quot;please install gguf==0.6.0&quot;)

from .utils import permute
from quark.shares.utils.log import ScreenLogger

logger = ScreenLogger(__name__)


class QuantSpec(object):

    def __init__(self,
                 tensor_name: str,
                 tensor: torch.Tensor,
                 scales: torch.Tensor | None = None,
                 zero_points: torch.Tensor | None = None,
                 quant_type: gguf.GGMLQuantizationType = None) -&gt; None:
        self.tensor_name = tensor_name
        self.tensor = tensor
        self.scales = scales
        self.zero_points = zero_points
        self.quant_type = quant_type


<div class="viewcode-block" id="SentencePieceTokenTypes">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/export/gguf_export/gguf_model_writer/index.html#quark.torch.export.gguf_export.gguf_model_writer.SentencePieceTokenTypes">[docs]</a>
class SentencePieceTokenTypes(IntEnum):
    NORMAL = 1
    UNKNOWN = 2
    CONTROL = 3
    USER_DEFINED = 4
    UNUSED = 5
    BYTE = 6</div>



AnyModel = TypeVar(&quot;AnyModel&quot;, bound=&quot;type[ModelWriter]&quot;)


<div class="viewcode-block" id="ModelWriter">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/export/gguf_export/gguf_model_writer/index.html#quark.torch.export.gguf_export.gguf_model_writer.ModelWriter">[docs]</a>
class ModelWriter(ABC):
    _model_classes: dict[str, type[ModelWriter]] = {}

    def __init__(self,
                 model_name: str,
                 json_path: Path,
                 safetensor_path: Path,
                 tokenizer_dir: Path,
                 fname_out: Path,
                 is_big_endian: bool = False,
                 use_temp_file: bool = False):
        self.model_name = model_name
        with open(json_path, &#39;r&#39;) as f:
            self._model_json: Dict[str, int | str | Any] = json.load(f)
        self.safetensor_path = safetensor_path
        self.tokenizer_dir = tokenizer_dir
        self.fname_out = fname_out
        self.is_big_endian = is_big_endian
        self.endianess = gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE
        self.use_temp_file = use_temp_file
        self.hparams = self.load_hparams()
        self.gguf_writer = gguf.GGUFWriter(fname_out,
                                           gguf.MODEL_ARCH_NAMES[self.model_arch],
                                           endianess=self.endianess,
                                           use_temp_file=self.use_temp_file)
        self.block_count = self.find_hparam([&quot;n_layers&quot;, &quot;num_hidden_layers&quot;, &quot;n_layer&quot;])

    def parse_quant_info(self) -&gt; Dict[str, Dict[str, str | int]]:
        quant_info = {}

        def traverse_model_json(model_json: Dict[str, Any]) -&gt; None:
            for k, v in model_json.items():
                if not isinstance(v, dict):
                    continue
                if &quot;weight_quant&quot; in v:
                    quant_info[v[&quot;weight&quot;]] = v[&quot;weight_quant&quot;]
                else:
                    traverse_model_json(v)

        traverse_model_json(self._model_json)
        return quant_info

    def get_quant_type_from_weight_quant(self, weight_quant: Dict[str, str | int]) -&gt; gguf.GGMLQuantizationType:
        if weight_quant[&quot;dtype&quot;] == &quot;uint4&quot; and weight_quant[&quot;qscheme&quot;] == &quot;per_group&quot; and weight_quant[
                &quot;group_size&quot;] == 32:
            return gguf.GGMLQuantizationType.Q4_1
        else:
            raise Exception(&quot;Unsupported quant spec&quot;)

    def is_quant_spec_complete(self, quant_spec: QuantSpec) -&gt; bool:
        return quant_spec.tensor is not None and quant_spec.scales is not None and quant_spec.zero_points is not None

    @property
    @abstractmethod
    def model_arch(self) -&gt; gguf.MODEL_ARCH:
        pass

    def find_hparam(self, keys: Sequence[str], optional: bool = False) -&gt; Any:
        key = next((k for k in keys if k in self.hparams), None)
        if key is not None:
            return self.hparams[key]
        if optional:
            return None
        raise KeyError(f&quot;could not find any of: {keys}&quot;)

    def set_vocab(self) -&gt; None:
        pass

    def get_tensors(self) -&gt; Iterator[tuple[str, torch.Tensor]]:
        ctx: ContextManager[Any]
        from safetensors import safe_open
        ctx = cast(ContextManager[Any], safe_open(self.safetensor_path, framework=&quot;pt&quot;, device=&quot;cpu&quot;))

        with ctx as model_part:
            for name in model_part.keys():
                data = model_part.get_tensor(name)
                yield name, data

    def set_gguf_parameters(self) -&gt; None:
        self.gguf_writer.add_name(self.model_name)
        self.gguf_writer.add_block_count(self.block_count)

        if (n_ctx := self.find_hparam([&quot;max_position_embeddings&quot;, &quot;n_ctx&quot;], optional=True)) is not None:
            self.gguf_writer.add_context_length(n_ctx)
            logger.info(f&quot;gguf: context length = {n_ctx}&quot;)

        n_embd = self.find_hparam([&quot;hidden_size&quot;, &quot;n_embd&quot;])
        self.gguf_writer.add_embedding_length(n_embd)
        logger.info(f&quot;gguf: embedding length = {n_embd}&quot;)

        if (n_ff := self.find_hparam([&quot;intermediate_size&quot;, &quot;n_inner&quot;], optional=True)) is not None:
            self.gguf_writer.add_feed_forward_length(n_ff)
            logger.info(f&quot;gguf: feed forward length = {n_ff}&quot;)

        n_head = self.find_hparam([&quot;num_attention_heads&quot;, &quot;n_head&quot;])
        self.gguf_writer.add_head_count(n_head)
        logger.info(f&quot;gguf: head count = {n_head}&quot;)

        if (n_head_kv := self.hparams.get(&quot;num_key_value_heads&quot;)) is not None:
            self.gguf_writer.add_head_count_kv(n_head_kv)
            logger.info(f&quot;gguf: key-value head count = {n_head_kv}&quot;)

        if (rope_theta := self.hparams.get(&quot;rope_theta&quot;)) is not None:
            self.gguf_writer.add_rope_freq_base(rope_theta)
            logger.info(f&quot;gguf: rope theta = {rope_theta}&quot;)
        if (f_rms_eps := self.hparams.get(&quot;rms_norm_eps&quot;)) is not None:
            self.gguf_writer.add_layer_norm_rms_eps(f_rms_eps)
            logger.info(f&quot;gguf: rms norm epsilon = {f_rms_eps}&quot;)
        if (f_norm_eps := self.find_hparam([&quot;layer_norm_eps&quot;, &quot;layer_norm_epsilon&quot;, &quot;norm_epsilon&quot;],
                                           optional=True)) is not None:
            self.gguf_writer.add_layer_norm_eps(f_norm_eps)
            logger.info(f&quot;gguf: layer norm epsilon = {f_norm_eps}&quot;)
        if (n_experts := self.hparams.get(&quot;num_local_experts&quot;)) is not None:
            self.gguf_writer.add_expert_count(n_experts)
            logger.info(f&quot;gguf: expert count = {n_experts}&quot;)
        if (n_experts_used := self.hparams.get(&quot;num_experts_per_tok&quot;)) is not None:
            self.gguf_writer.add_expert_used_count(n_experts_used)
            logger.info(f&quot;gguf: experts used count = {n_experts_used}&quot;)

        self.gguf_writer.add_file_type(gguf.GGMLQuantizationType.F32)

    @abstractmethod
    def write_tensors(self) -&gt; None:
        pass

    def write(self) -&gt; None:
        self.write_tensors()
        self.gguf_writer.write_header_to_file()
        self.gguf_writer.write_kv_data_to_file()
        self.gguf_writer.write_tensors_to_file()
        self.gguf_writer.close()

    def write_vocab(self) -&gt; None:
        self.gguf_writer.write_header_to_file()
        self.gguf_writer.write_kv_data_to_file()
        self.gguf_writer.close()

    @staticmethod
    def count_model_parts(dir_model: Path, prefix: str) -&gt; int:
        num_parts = 0
        for filename in os.listdir(dir_model):
            if filename.endswith(prefix):
                num_parts += 1

        return num_parts

    def load_hparams(self) -&gt; Dict[str, int | str | Any]:
        return self._model_json[&quot;config&quot;]  # type: ignore[return-value]

    @classmethod
    def register(cls, *names: str) -&gt; Callable[[type[ModelWriter]], type[ModelWriter]]:
        assert names

        def func(modelcls: type[ModelWriter]) -&gt; type[ModelWriter]:
            for name in names:
                cls._model_classes[name] = modelcls
            return modelcls

        return func

    @classmethod
    @log_errors
    def from_model_architecture(cls: type[ModelWriter], arch: str) -&gt; type[ModelWriter]:
        try:
            return cls._model_classes[arch]
        except KeyError as e:
            raise NotImplementedError(f&#39;Architecture {arch!r} not supported!&#39;)

    # used for GPT-2 BPE and WordPiece vocabs
    def get_vocab_base(self) -&gt; tuple[list[str], list[int], str]:
        tokens: list[str] = []
        toktypes: list[int] = []

        if not is_transformers_available():  # pragma: no cover
            raise ImportError(
                &quot;The `transformers` library is required to run `ModelWriter.get_vocab_base`, but the library was not found in the current environment. Please install Transformers library (`pip install transformers`).&quot;
            )
        tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_dir)
        vocab_size = int(self.hparams.get(&quot;vocab_size&quot;, len(tokenizer.vocab)))
        assert max(tokenizer.vocab.values()) &lt; vocab_size

        tokpre = self.get_vocab_base_pre(tokenizer)

        reverse_vocab = {id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()}
        added_vocab = tokenizer.get_added_vocab()

        for i in range(vocab_size):
            if i not in reverse_vocab:
                tokens.append(f&quot;[PAD{i}]&quot;)
                toktypes.append(gguf.TokenType.USER_DEFINED)
            elif reverse_vocab[i] in added_vocab:
                tokens.append(reverse_vocab[i])
                if tokenizer.added_tokens_decoder[i].special:
                    toktypes.append(gguf.TokenType.CONTROL)
                else:
                    toktypes.append(gguf.TokenType.USER_DEFINED)
            else:
                tokens.append(reverse_vocab[i])
                toktypes.append(gguf.TokenType.NORMAL)

        return tokens, toktypes, tokpre

    # NOTE: this function is generated by convert-hf-to-gguf-update.py
    #       do not modify it manually!
    # ref:  https://github.com/ggerganov/llama.cpp/pull/6920
    def get_vocab_base_pre(self, tokenizer) -&gt; str:  # type: ignore[no-untyped-def]
        # encoding this string and hashing the resulting tokens would (hopefully) give us a unique identifier that
        # is specific for the BPE pre-tokenizer used by the model
        # we will use this unique identifier to write a &quot;tokenizer.ggml.pre&quot; entry in the GGUF file which we can
        # use in llama.cpp to implement the same pre-tokenizer

        chktxt = &#39;\n \n\n \n\n\n \t \t\t \t\n  \n   \n    \n     \nüöÄ (normal) üò∂\u200düå´Ô∏è (multiple emojis concatenated) ‚úÖ ü¶ôü¶ô 3 33 333 3333 33333 333333 3333333 33333333 3.3 3..3 3...3 ·ûÄ·û∂·ûì·üã·ûè·üÇ·ûñ·û∑·ûü·üÅ·ûü·û¢·û∂·ûÖüòÅ ?ÊàëÊÉ≥Âú®appleÂ∑•‰Ωú1314151Â§©ÔΩû ------======= –Ω–µ—â–æ –Ω–∞ –ë—ä–ª–≥–∞—Ä—Å–∫–∏ \&#39;\&#39;\&#39;\&#39;\&#39;\&#39;```````&quot;&quot;&quot;&quot;......!!!!!!?????? I\&#39;ve been \&#39;told he\&#39;s there, \&#39;RE you sure? \&#39;M not sure I\&#39;ll make it, \&#39;D you like some tea? We\&#39;Ve a\&#39;lL&#39;

        chktok = tokenizer.encode(chktxt)
        chkhsh = sha256(str(chktok).encode()).hexdigest()
        res = None

        # NOTE: if you get an error here, you need to update the convert-hf-to-gguf-update.py script
        #       or pull the latest version of the model from Huggingface
        #       don&#39;t edit the hashes manually!
        if chkhsh == &quot;0ef9807a4087ebef797fc749390439009c3b9eda9ad1a097abbe738f486c01e5&quot;:
            # ref: https://huggingface.co/meta-llama/Meta-Llama-3-8B
            res = &quot;llama-bpe&quot;
        if chkhsh == &quot;049ecf7629871e3041641907f3de7c733e4dbfdc736f57d882ba0b0845599754&quot;:
            # ref: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base
            res = &quot;deepseek-llm&quot;
        if chkhsh == &quot;347715f544604f9118bb75ed199f68779f423cabb20db6de6f31b908d04d7821&quot;:
            # ref: https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base
            res = &quot;deepseek-coder&quot;
        if chkhsh == &quot;8aeee3860c56296a157a1fe2fad249ec40aa59b1bb5709f4ade11c4e6fe652ed&quot;:
            # ref: https://huggingface.co/tiiuae/falcon-7b
            res = &quot;falcon&quot;
        if chkhsh == &quot;0876d13b50744004aa9aeae05e7b0647eac9d801b5ba4668afc01e709c15e19f&quot;:
            # ref: https://huggingface.co/BAAI/bge-small-en-v1.5
            res = &quot;bert-bge&quot;
        if chkhsh == &quot;b6dc8df998e1cfbdc4eac8243701a65afe638679230920b50d6f17d81c098166&quot;:
            # ref: https://huggingface.co/mosaicml/mpt-7b
            res = &quot;mpt&quot;
        if chkhsh == &quot;35d91631860c815f952d711435f48d356ebac988362536bed955d43bfa436e34&quot;:
            # ref: https://huggingface.co/bigcode/starcoder2-3b
            res = &quot;starcoder&quot;
        if chkhsh == &quot;3ce83efda5659b07b1ad37ca97ca5797ea4285d9b9ab0dc679e4a720c9da7454&quot;:
            # ref: https://huggingface.co/openai-community/gpt2
            res = &quot;gpt-2&quot;
        if chkhsh == &quot;6221ad2852e85ce96f791f476e0b390cf9b474c9e3d1362f53a24a06dc8220ff&quot;:
            # ref: https://huggingface.co/smallcloudai/Refact-1_6-base
            res = &quot;refact&quot;
        if chkhsh == &quot;9c2227e4dd922002fb81bde4fc02b0483ca4f12911410dee2255e4987644e3f8&quot;:
            # ref: https://huggingface.co/CohereForAI/c4ai-command-r-v01
            res = &quot;command-r&quot;

        if res is None:
            logger.warning(&quot;\n&quot;)
            logger.warning(&quot;**************************************************************************************&quot;)
            logger.warning(&quot;** WARNING: The BPE pre-tokenizer was not recognized!&quot;)
            logger.warning(&quot;**          There are 2 possible reasons for this:&quot;)
            logger.warning(&quot;**          - the model has not been added to convert-hf-to-gguf-update.py yet&quot;)
            logger.warning(&quot;**          - the pre-tokenization config has changed upstream&quot;)
            logger.warning(
                &quot;**          Check your model files and convert-hf-to-gguf-update.py and update them accordingly.&quot;)
            logger.warning(&quot;** ref:     https://github.com/ggerganov/llama.cpp/pull/6920&quot;)
            logger.warning(&quot;**&quot;)
            logger.warning(f&quot;** chkhsh:  {chkhsh}&quot;)
            logger.warning(&quot;**************************************************************************************&quot;)
            logger.warning(&quot;\n&quot;)
            raise NotImplementedError(&quot;BPE pre-tokenizer was not recognized - update get_vocab_base_pre()&quot;)

        return res

    def _set_vocab_sentencepiece(self) -&gt; None:
        from sentencepiece import SentencePieceProcessor  # type: ignore

        tokenizer_path = self.tokenizer_dir / &#39;tokenizer.model&#39;

        tokens: list[bytes | str] = []
        scores: list[float] = []
        toktypes: list[int] = []

        if not tokenizer_path.is_file():
            raise FileNotFoundError(f&quot;File not found: {tokenizer_path}&quot;)

        tokenizer = SentencePieceProcessor(str(tokenizer_path))
        vocab_size = int(self.hparams.get(&#39;vocab_size&#39;, tokenizer.vocab_size()))

        for token_id in range(tokenizer.vocab_size()):
            piece = tokenizer.id_to_piece(token_id)
            text = piece.encode(&quot;utf-8&quot;)
            score = tokenizer.get_score(token_id)

            toktype = SentencePieceTokenTypes.NORMAL
            if tokenizer.is_unknown(token_id):
                toktype = SentencePieceTokenTypes.UNKNOWN
            elif tokenizer.is_control(token_id):
                toktype = SentencePieceTokenTypes.CONTROL
            elif tokenizer.is_unused(token_id):
                toktype = SentencePieceTokenTypes.UNUSED
            elif tokenizer.is_byte(token_id):
                toktype = SentencePieceTokenTypes.BYTE

            tokens.append(text)
            scores.append(score)
            toktypes.append(toktype)

        added_tokens_file = self.tokenizer_dir / &#39;added_tokens.json&#39;
        if added_tokens_file.is_file():
            with open(added_tokens_file, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f:
                added_tokens_json = json.load(f)

                for key in added_tokens_json:
                    key = key.encode(&quot;utf-8&quot;)
                    if key not in tokens:
                        tokens.append(key)
                        scores.append(-1000.0)
                        toktypes.append(SentencePieceTokenTypes.USER_DEFINED)

        if vocab_size &gt; len(tokens):
            pad_count = vocab_size - len(tokens)
            for i in range(1, pad_count + 1):
                tokens.append(f&quot;[PAD{i}]&quot;)
                scores.append(-1000.0)
                toktypes.append(SentencePieceTokenTypes.UNUSED)

        assert len(tokens) == vocab_size

        self.gguf_writer.add_tokenizer_model(&quot;llama&quot;)
        self.gguf_writer.add_string(&quot;tokenizer.ggml.pre&quot;, &quot;default&quot;)
        self.gguf_writer.add_token_list(tokens)
        self.gguf_writer.add_token_scores(scores)
        self.gguf_writer.add_token_types(toktypes)

        special_vocab = gguf.SpecialVocab(self.tokenizer_dir, n_vocab=len(tokens))
        special_vocab.add_to_gguf(self.gguf_writer)</div>



@ModelWriter.register(&quot;LlamaForCausalLM&quot;, &quot;MistralForCausalLM&quot;, &quot;MixtralForCausalLM&quot;)
<div class="viewcode-block" id="LlamaModelWriter">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/export/gguf_export/gguf_model_writer/index.html#quark.torch.export.gguf_export.gguf_model_writer.LlamaModelWriter">[docs]</a>
class LlamaModelWriter(ModelWriter):
    model_arch = gguf.MODEL_ARCH.LLAMA

    def set_vocab(self) -&gt; None:
        self._set_vocab_sentencepiece()

        # Apply to CodeLlama only (and ignore for Llama 3 with a vocab size of 128256)
        if self.hparams.get(&quot;vocab_size&quot;, 32000) == 32016:
            special_vocab = gguf.SpecialVocab(self.tokenizer_dir,
                                              load_merges=False,
                                              special_token_types=[&#39;prefix&#39;, &#39;suffix&#39;, &#39;middle&#39;, &#39;eot&#39;])
            special_vocab._set_special_token(&quot;prefix&quot;, 32007)
            special_vocab._set_special_token(&quot;suffix&quot;, 32008)
            special_vocab._set_special_token(&quot;middle&quot;, 32009)
            special_vocab._set_special_token(&quot;eot&quot;, 32010)
            special_vocab.add_to_gguf(self.gguf_writer)

    def set_gguf_parameters(self) -&gt; None:
        super().set_gguf_parameters()
        hparams = self.hparams
        self.gguf_writer.add_uint32(f&quot;{self.gguf_writer.arch}.vocab_size&quot;, hparams[&quot;vocab_size&quot;])
        self.gguf_writer.add_rope_dimension_count(hparams[&quot;hidden_size&quot;] //  # type: ignore
                                                  hparams[&quot;num_attention_heads&quot;])

        if self.hparams.get(&quot;rope_scaling&quot;) is not None and &quot;factor&quot; in self.hparams[&quot;rope_scaling&quot;]:  # type: ignore
            if self.hparams[&quot;rope_scaling&quot;].get(&quot;type&quot;) == &quot;linear&quot;:  # type: ignore
                self.gguf_writer.add_rope_scaling_type(gguf.RopeScalingType.LINEAR)
                self.gguf_writer.add_rope_scaling_factor(self.hparams[&quot;rope_scaling&quot;][&quot;factor&quot;])  # type: ignore

    # Same as super class, but permuting q_proj, k_proj
    def write_tensors(self) -&gt; None:
        block_count = int(
            self.hparams.get(&quot;n_layers&quot;, self.hparams.get(&quot;num_hidden_layers&quot;, self.hparams.get(&quot;n_layer&quot;))))
        tensor_map = gguf.get_tensor_name_map(self.model_arch, block_count)
        n_head = int(self.hparams.get(&quot;num_attention_heads&quot;))  # type: ignore
        n_kv_head = int(self.hparams.get(&quot;num_key_value_heads&quot;))  # type: ignore

        quant_info = self.parse_quant_info()
        quant_spec_map: Dict[str, QuantSpec] = {}
        for name, data_torch in self.get_tensors():
            # we don&#39;t need these
            if name.endswith((&quot;.attention.masked_bias&quot;, &quot;.attention.bias&quot;, &quot;.rotary_emb.inv_freq&quot;)):
                continue
            # convert any unsupported data types to float32
            if data_torch.dtype not in (torch.float16, torch.float32):
                data_torch = data_torch.to(torch.float32)

            n_dims = len(data_torch.shape)
            if n_dims == 1:
                data_torch = data_torch.to(torch.float32)

            raw_shape = None
            raw_dtype = None
            if name in quant_info:
                quant_spec_map[name] = QuantSpec(tensor_name=name,
                                                 tensor=data_torch,
                                                 scales=None,
                                                 zero_points=None,
                                                 quant_type=self.get_quant_type_from_weight_quant(quant_info[name]))
                continue
            if name.endswith(&quot;_scale&quot;):
                tensor_name = name[:-len(&quot;_scale&quot;)]
                assert tensor_name in quant_spec_map, f&quot;tensor : {tensor_name} has to be in quant_spec_map&quot;
                quant_spec = quant_spec_map[tensor_name]
                quant_spec.scales = data_torch
                if self.is_quant_spec_complete(quant_spec):
                    data_torch = convert_to_gguf(
                        inpt=quant_spec.tensor,
                        scale=quant_spec.scales,
                        zero_point=quant_spec.zero_points,  # type: ignore
                        gguf_type=quant_spec.quant_type)
                    raw_shape = quant_spec.tensor.shape
                    raw_dtype = quant_spec.quant_type
                    name = quant_spec.tensor_name
                else:
                    continue
            if name.endswith(&quot;_zero_point&quot;):
                tensor_name = name[:-len(&quot;_zero_point&quot;)]
                assert tensor_name in quant_spec_map, f&quot;tensor : {tensor_name} has to be in quant_spec_map&quot;
                quant_spec_map[tensor_name].zero_points = data_torch
                if self.is_quant_spec_complete(quant_spec):
                    data_torch = convert_to_gguf(
                        inpt=quant_spec.tensor,
                        scale=quant_spec.scales,  # type: ignore
                        zero_point=quant_spec.zero_points,  # type: ignore
                        gguf_type=quant_spec.quant_type)
                    raw_shape = quant_spec.tensor.shape
                    raw_dtype = quant_spec.quant_type
                    name = quant_spec.tensor_name
                else:
                    continue

            if name.endswith(&quot;q_proj.weight&quot;):
                data_torch = permute(data_torch, n_head, n_head)
            if name.endswith(&quot;k_proj.weight&quot;):
                data_torch = permute(data_torch, n_head, n_kv_head)

            data = data_torch.numpy()

            data = data.squeeze()

            # map tensor names
            new_name = tensor_map.get_name(name, try_suffixes=(&quot;.weight&quot;, &quot;.bias&quot;))
            if new_name is None:
                raise ValueError(f&quot;Can not map tensor {name!r}&quot;)

            self.gguf_writer.add_tensor(new_name, data, raw_shape=raw_shape, raw_dtype=raw_dtype)</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">¬© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>